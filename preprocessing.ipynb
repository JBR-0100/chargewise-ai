{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fda5ba53",
   "metadata": {},
   "source": [
    "# Milestone 1 — EV Charging Demand Prediction\n",
    "\n",
    "## 1. Problem Understanding\n",
    "\n",
    "**Objective:** Build a machine-learning / time-series forecasting system that predicts EV charging demand at stations using historical usage data from the **UrbanEVDataset** (Shenzhen, China — Sep 2022 to Feb 2023).\n",
    "\n",
    "### Use-Case Description\n",
    "EV charging stations in urban areas experience highly variable demand patterns driven by:\n",
    "- **Time of day** (rush hours vs. off-peak)\n",
    "- **Day of week** (weekdays vs. weekends)\n",
    "- **Season** (weather, holidays)\n",
    "- **Station location** (residential area vs. commercial)\n",
    "\n",
    "Accurate demand forecasting enables:\n",
    "- Grid operators to balance electricity load\n",
    "- Station managers to schedule maintenance\n",
    "- EV drivers to find available chargers\n",
    "\n",
    "### Input–Output Specification\n",
    "| Field | Description |\n",
    "|---|---|\n",
    "| **Inputs** | Historical 5-min interval station data (busy/idle piles, energy volume, price, duration) |\n",
    "| **Station Info** | GPS coordinates, pile counts, zone (TAZID) |\n",
    "| **Outputs** | Hourly energy demand forecast (kWh) per zone |\n",
    "| **Evaluation** | MAE, RMSE on held-out 20% test split |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e492c55",
   "metadata": {},
   "source": [
    "## 2. Forecasting Pipeline (System Architecture)\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────┐\n",
    "│                     FORECASTING PIPELINE                           │\n",
    "│                                                                     │\n",
    "│  Raw Station CSVs (5-min)                                           │\n",
    "│       │                                                             │\n",
    "│       ▼                                                             │\n",
    "│  Data Cleaning & Gap-Filling (ffill/bfill)                          │\n",
    "│       │                                                             │\n",
    "│       ▼                                                             │\n",
    "│  Remove Bad Stations (all-idle / header-only)                       │\n",
    "│       │                                                             │\n",
    "│       ▼                                                             │\n",
    "│  Hourly Aggregation (per-station → sum/mean)                        │\n",
    "│       │                                                             │\n",
    "│  Station Info CSV ──► Zone Mapping (station_id → TAZID)            │\n",
    "│       │                                                             │\n",
    "│       ▼                                                             │\n",
    "│  Zone-Level Hourly Demand (sum of station volumes per zone)         │\n",
    "│       │                                                             │\n",
    "│       ▼                                                             │\n",
    "│  Feature Engineering                                                │\n",
    "│     hour, dayofweek, month, season, is_weekend, lag_1, lag_24      │\n",
    "│       │                                                             │\n",
    "│       ▼                                                             │\n",
    "│  Model Training (Linear Regression)                          │\n",
    "│     Train: 80% | Test: 20% (time-ordered)                           │\n",
    "│       │                                                             │\n",
    "│       ▼                                                             │\n",
    "│  Evaluation: MAE, RMSE per zone                                     │\n",
    "│       │                                                             │\n",
    "│       ▼                                                             │\n",
    "│  Streamlit Dashboard (demand forecast + peak detection)             │\n",
    "└─────────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23c3d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib .pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "warnings .filterwarnings ('ignore')\n",
    "from sklearn .linear_model import LinearRegression\n",
    "from sklearn .metrics import mean_absolute_error ,mean_squared_error\n",
    "from sklearn .model_selection import train_test_split\n",
    "BASE_DIR =os .path .dirname (os .path .abspath ('__file__'))if '__file__'in dir ()else os .getcwd ()\n",
    "RAW_DIR =os .path .join (BASE_DIR ,'20220901-20230228_station-raw')\n",
    "CLEAN_DIR =os .path .join (BASE_DIR ,'processed')\n",
    "STATION_INFO_PATH =os .path .join (RAW_DIR ,'station_information.csv')\n",
    "RAW_5MIN_GLOB =os .path .join (RAW_DIR ,'charge_5min','*.csv')\n",
    "OUT_5MIN =os .path .join (CLEAN_DIR ,'charge_5min')\n",
    "OUT_1HOUR =os .path .join (CLEAN_DIR ,'charge_1hour')\n",
    "OUT_ZONE =os .path .join (CLEAN_DIR ,'zone_hourly_volume_long.csv')\n",
    "os .makedirs (OUT_5MIN ,exist_ok =True )\n",
    "os .makedirs (OUT_1HOUR ,exist_ok =True )\n",
    "print (' Directories ready')\n",
    "print (f'   RAW:     {RAW_DIR }')\n",
    "print (f'   OUTPUT:  {CLEAN_DIR }')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a84541",
   "metadata": {},
   "source": [
    "## 3. Station Information & Zone Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ccb690",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_info =pd .read_csv (STATION_INFO_PATH )\n",
    "print (f'Stations: {len (station_info )}')\n",
    "print (f'Zones covered: {station_info [\"TAZID\"].nunique ()}')\n",
    "station_info .head ()\n",
    "station_info .describe ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bdb721",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_to_zone =dict (zip (station_info ['station_id'],station_info ['TAZID']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dabdc16",
   "metadata": {},
   "source": [
    "## 4. Finding Bad Stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364e7de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is how we found the bad stations in kaggle before shifting to local\n",
    "\n",
    "input_folder = '/kaggle/input/datasets/aayushchaturvedii/urbanevdataset/UrbanEVDataset/20220901-20230228_station-raw/charge_5min/*.csv'\n",
    "output_folder = '/kaggle/working/charge_5min'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "files = glob.glob(input_folder)\n",
    "# files = ['/kaggle/input/datasets/aayushchaturvedii/urbanevdataset/UrbanEVDataset/20220901-20230228_station-raw/charge_5min/1523.csv']\n",
    "\n",
    "for file_path in files:\n",
    "    df = pd.read_csv(file_path)\n",
    "    station_id = int(file_path.split(\"/\")[-1].replace(\".csv\", \"\"))\n",
    "    zone_id = station_to_zone[station_id]\n",
    "    df[\"TAZID\"] = zone_id\n",
    "    df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
    "    \n",
    "    df = df.sort_values(\"time\").reset_index(drop=True).set_index('time')\n",
    "\n",
    "    full_index = pd.date_range(\n",
    "        start=df.index.min(),\n",
    "        end=df.index.max(),\n",
    "        freq=\"5min\"\n",
    "    )\n",
    "    df = df.reindex(full_index)\n",
    "    \n",
    "    df = df.ffill().bfill()\n",
    "    \n",
    "    df = df.reset_index().rename(columns={\"index\": \"time\"})\n",
    "\n",
    "    if (df.isna().sum().sum() == 0):\n",
    "        print(\"Success\")\n",
    "    else:\n",
    "        print(file_path,\"Fail\")\n",
    "        \n",
    "    output_path = os.path.join(output_folder, f\"{station_id}.csv\")\n",
    "    df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69412f55",
   "metadata": {},
   "source": [
    "## 5. Preprocessing — All Stations (5-min cleaning + gap fill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fc3b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "BAD_STATIONS ={2129 ,1663 ,1478 ,1082 ,1055 ,1722 ,1039 ,1036 ,1681 ,2125 ,1487 ,1113 ,2138 ,1034 ,1337 ,1497 ,2337 ,1501 ,1101 ,2291 }\n",
    "EXPECTED_5MIN_FREQ ='5min'\n",
    "files =sorted (glob .glob (RAW_5MIN_GLOB ))\n",
    "print (f'Total raw station files: {len (files )}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3200fdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "success ,skipped ,fail =[],[],[]\n",
    "for file_path in files :\n",
    "    station_id =int (os .path .basename (file_path ).replace ('.csv',''))\n",
    "    if station_id in BAD_STATIONS :\n",
    "        skipped .append (station_id )\n",
    "        continue\n",
    "    if station_id not in station_to_zone :\n",
    "        skipped .append (station_id )\n",
    "        continue\n",
    "    zone_id =station_to_zone [station_id ]\n",
    "    try :\n",
    "        df =pd .read_csv (file_path )\n",
    "        if len (df )<10 :\n",
    "            skipped .append (station_id )\n",
    "            continue\n",
    "        df ['time']=pd .to_datetime (df ['time'])\n",
    "        df ['TAZID']=zone_id\n",
    "        df =df .sort_values ('time').set_index ('time')\n",
    "        full_index =pd .date_range (start =df .index .min (),end =df .index .max (),freq =EXPECTED_5MIN_FREQ )\n",
    "        df =df .reindex (full_index )\n",
    "        df =df .ffill ().bfill ()\n",
    "        df =df .reset_index ().rename (columns ={'index':'time'})\n",
    "        null_count =df .isnull ().sum ().sum ()\n",
    "        if null_count >0 :\n",
    "            fail .append (station_id )\n",
    "            continue\n",
    "        out_path =os .path .join (OUT_5MIN ,f'{station_id }.csv')\n",
    "        df .to_csv (out_path ,index =False )\n",
    "        success .append (station_id )\n",
    "    except Exception as e :\n",
    "        print (f'   Station {station_id }: {e }')\n",
    "        fail .append (station_id )\n",
    "print (f'\\n Cleaned: {len (success )} stations')\n",
    "print (f'⏭  Skipped (bad/unmapped): {len (skipped )} stations')\n",
    "print (f' Failed: {len (fail )} stations')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c18fd3f",
   "metadata": {},
   "source": [
    "## 6. Preprocessing — Hourly Aggregation (per-station)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bde453f",
   "metadata": {},
   "outputs": [],
   "source": [
    "AGG_RULES ={'busy':'mean','idle':'mean','fast_busy':'mean','fast_idle':'mean','slow_busy':'mean','slow_idle':'mean','duration':'sum','volume':'sum','s_price':'mean','e_price':'mean','TAZID':'first',}\n",
    "cleaned_files =sorted (glob .glob (os .path .join (OUT_5MIN ,'*.csv')))\n",
    "print (f'Aggregating {len (cleaned_files )} stations to hourly …')\n",
    "for file_path in cleaned_files :\n",
    "    station_id =int (os .path .basename (file_path ).replace ('.csv',''))\n",
    "    df =pd .read_csv (file_path )\n",
    "    df ['time']=pd .to_datetime (df ['time'])\n",
    "    df =df .set_index ('time')\n",
    "    df_hourly =df .resample ('h').agg (AGG_RULES ).reset_index ()\n",
    "    out_path =os .path .join (OUT_1HOUR ,f'{station_id }.csv')\n",
    "    df_hourly .to_csv (out_path ,index =False )\n",
    "hourly_files =glob .glob (os .path .join (OUT_1HOUR ,'*.csv'))\n",
    "print (f' Hourly files written: {len (hourly_files )}')\n",
    "sample_h =pd .read_csv (hourly_files [0 ])\n",
    "print (f'   Sample shape: {sample_h .shape } — columns: {list (sample_h .columns )}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e014ffe",
   "metadata": {},
   "source": [
    "## 7. Zone-Level Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b6e63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data =[]\n",
    "for file_path in hourly_files :\n",
    "    df =pd .read_csv (file_path )\n",
    "    df ['time']=pd .to_datetime (df ['time'])\n",
    "    all_data .append (df [['time','TAZID','volume']])\n",
    "all_data =pd .concat (all_data ,ignore_index =True )\n",
    "zone_hourly =(all_data .groupby (['time','TAZID'],as_index =False ).agg ({'volume':'sum'}))\n",
    "zone_hourly .to_csv (OUT_ZONE ,index =False )\n",
    "print (f'Zone-hourly dataset: {zone_hourly .shape }')\n",
    "print (f'Zones: {zone_hourly [\"TAZID\"].nunique ()}')\n",
    "print (f'Time range: {zone_hourly [\"time\"].min ()} → {zone_hourly [\"time\"].max ()}')\n",
    "zone_hourly .head ()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2e1d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Volume statistics (kWh) across all zones and hours:')\n",
    "zone_hourly ['volume'].describe ()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff131a53",
   "metadata": {},
   "source": [
    "## 8. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5eb1096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features (df :pd .DataFrame )->pd .DataFrame :\n",
    "    df =df .copy ()\n",
    "    df ['hour']=df .index .hour\n",
    "    df ['dayofweek']=df .index .dayofweek\n",
    "    df ['month']=df .index .month\n",
    "    df ['is_weekend']=(df .index .dayofweek >=5 ).astype (int )\n",
    "    df ['season']=df ['month'].map ({12 :0 ,1 :0 ,2 :0 ,3 :1 ,4 :1 ,5 :1 ,6 :2 ,7 :2 ,8 :2 ,9 :3 ,10 :3 ,11 :3 })\n",
    "    df ['hour_sin']=np .sin (2 *np .pi *df ['hour']/24 )\n",
    "    df ['hour_cos']=np .cos (2 *np .pi *df ['hour']/24 )\n",
    "    df ['dow_sin']=np .sin (2 *np .pi *df ['dayofweek']/7 )\n",
    "    df ['dow_cos']=np .cos (2 *np .pi *df ['dayofweek']/7 )\n",
    "    df ['lag_1h']=df ['volume'].shift (1 )\n",
    "    df ['lag_24h']=df ['volume'].shift (24 )\n",
    "    df ['lag_168h']=df ['volume'].shift (168 )\n",
    "    df ['roll_24h_mean']=df ['volume'].shift (1 ).rolling (24 ).mean ()\n",
    "    return df .dropna ()\n",
    "FEATURES =['hour','dayofweek','month','is_weekend','season','hour_sin','hour_cos','dow_sin','dow_cos','lag_1h','lag_24h','lag_168h','roll_24h_mean']\n",
    "print ('Feature set:')\n",
    "for f in FEATURES :\n",
    "    print (f'  · {f }')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ff2552",
   "metadata": {},
   "source": [
    "# 9. Model Training Per Zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377d9904",
   "metadata": {},
   "outputs": [],
   "source": [
    "zone_hourly_df = pd.read_csv(OUT_ZONE)\n",
    "zone_hourly_df[\"time\"] = pd.to_datetime(zone_hourly_df[\"time\"])\n",
    "zones = zone_hourly_df[\"TAZID\"].unique()\n",
    "results = []\n",
    "evaluated = 0\n",
    "for zone_id in zones:\n",
    "    zdf = (\n",
    "        zone_hourly_df[zone_hourly_df[\"TAZID\"] == zone_id]\\\n",
    "        .copy().sort_values(\"time\").set_index(\"time\")\\\n",
    "    )\n",
    "    zdf = add_features(zdf)\n",
    "    if len(zdf) < 200:\n",
    "        continue\n",
    "    X, y = zdf[FEATURES], zdf[\"volume\"]\n",
    "    split = int(len(X) * 0.8)\n",
    "    X_tr, X_te = X.iloc[:split], X.iloc[split:]\n",
    "    y_tr, y_te = y.iloc[:split], y.iloc[split:]\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_tr, y_tr)\n",
    "    y_pred = np.maximum(model.predict(X_te), 0)\n",
    "    mae  = mean_absolute_error(y_te, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_te, y_pred))\n",
    "    results.append({\"zone\": zone_id, \"model\": \"LinearRegression\", \"MAE\": mae, \"RMSE\": rmse})\n",
    "    evaluated += 1\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(OUT_RESULTS, index=False)\n",
    "print(f\"   Evaluated {evaluated} zones\")\n",
    "print(f\"\\n   Mean performance across all zones \")\n",
    "print(results_df[[\"MAE\", \"RMSE\"]].mean().round(3).to_string())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
